{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277fc8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rajat shukla\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bbcf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbcb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6191390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAJATS~1\\AppData\\Local\\Temp/ipykernel_1504/2880887014.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\Rajat Shukla\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Rajat Shukla\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67ce72",
   "metadata": {},
   "source": [
    "#Q1- Write a python program which searches all the product under a particular product vertical from www.amazon.in. The product verticals to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad913148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAJATS~1\\AppData\\Local\\Temp/ipykernel_1504/1892044109.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(\"chromedriver.exe\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search : Guitar\n"
     ]
    }
   ],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(1)\n",
    "\n",
    "# Opening the homepage of Amazon.in\n",
    "url = (\"https://www.amazon.in/\")\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(1)\n",
    "# Asking the user to input the keywords he/she wants to search\n",
    "user_inp = input('Enter the product you want to search : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc1f43a",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=106.0.5249.119)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x006CDF13+2219795]\n\tOrdinal0 [0x00662841+1779777]\n\tOrdinal0 [0x00574100+803072]\n\tOrdinal0 [0x00569BF2+760818]\n\tOrdinal0 [0x0055CD35+707893]\n\tOrdinal0 [0x00561122+725282]\n\tOrdinal0 [0x00565699+743065]\n\tOrdinal0 [0x00575610+808464]\n\tOrdinal0 [0x005CE93A+1173818]\n\tOrdinal0 [0x005BE616+1107478]\n\tOrdinal0 [0x00597F89+950153]\n\tOrdinal0 [0x00598F56+954198]\n\tGetHandleVerifier [0x009C2CB2+3040210]\n\tGetHandleVerifier [0x009B2BB4+2974420]\n\tGetHandleVerifier [0x00766A0A+565546]\n\tGetHandleVerifier [0x00765680+560544]\n\tOrdinal0 [0x00669A5C+1808988]\n\tOrdinal0 [0x0066E3A8+1827752]\n\tOrdinal0 [0x0066E495+1827989]\n\tOrdinal0 [0x006780A4+1867940]\n\tBaseThreadInitThunk [0x74E26359+25]\n\tRtlGetAppContainerNamedObjectPath [0x773587A4+228]\n\tRtlGetAppContainerNamedObjectPath [0x77358774+180]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAJATS~1\\AppData\\Local\\Temp/ipykernel_1504/406448199.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLASS_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"nav-search-field \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_inp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msearch_button\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//div[@class=\"nav-search-submit nav-sprite\"]/span/input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msearch_button\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    853\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'[name=\"%s\"]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0m\u001b[0;32m    856\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m             'value': value})['value']\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    430\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=106.0.5249.119)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x006CDF13+2219795]\n\tOrdinal0 [0x00662841+1779777]\n\tOrdinal0 [0x00574100+803072]\n\tOrdinal0 [0x00569BF2+760818]\n\tOrdinal0 [0x0055CD35+707893]\n\tOrdinal0 [0x00561122+725282]\n\tOrdinal0 [0x00565699+743065]\n\tOrdinal0 [0x00575610+808464]\n\tOrdinal0 [0x005CE93A+1173818]\n\tOrdinal0 [0x005BE616+1107478]\n\tOrdinal0 [0x00597F89+950153]\n\tOrdinal0 [0x00598F56+954198]\n\tGetHandleVerifier [0x009C2CB2+3040210]\n\tGetHandleVerifier [0x009B2BB4+2974420]\n\tGetHandleVerifier [0x00766A0A+565546]\n\tGetHandleVerifier [0x00765680+560544]\n\tOrdinal0 [0x00669A5C+1808988]\n\tOrdinal0 [0x0066E3A8+1827752]\n\tOrdinal0 [0x0066E495+1827989]\n\tOrdinal0 [0x006780A4+1867940]\n\tBaseThreadInitThunk [0x74E26359+25]\n\tRtlGetAppContainerNamedObjectPath [0x773587A4+228]\n\tRtlGetAppContainerNamedObjectPath [0x77358774+180]\n"
     ]
    }
   ],
   "source": [
    "search = driver.find_element(By.CLASS_NAME,\"nav-search-field \")    \n",
    "search.clear()                                               \n",
    "search.send_keys(user_inp)                                   \n",
    "search_button = driver.find_element_by_xpath('//div[@class=\"nav-search-submit nav-sprite\"]/span/input')\n",
    "search_button.click()                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea92c33",
   "metadata": {},
   "source": [
    "Q2- In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a dataframe and csv. In case if any product vertical has less than 3 pages in search results then scrape all the products available under that product vertical. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d632aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the urls to open each page from our above search results\n",
    "urls =[]\n",
    "for i in range(0,3):\n",
    "    for x in driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\"):\n",
    "        urls.append(x.get_attribute('href'))\n",
    "    driver.find_element_by_xpath(\"//li[@class='a-last']/a\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "# all the initial empty lists\n",
    "brand=[]\n",
    "prod_name=[]\n",
    "rating=[]\n",
    "rating_num=[]\n",
    "price= []\n",
    "ret_ex=[]\n",
    "exp_del = []\n",
    "availability=[]\n",
    "prod_url=[]\n",
    "ot_det = []\n",
    "\n",
    "# obtaining all the details one after another\n",
    "for i in urls[:150]:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # getting the brand name\n",
    "    try:\n",
    "        brand_name=driver.find_element_by_xpath(\"//td[@class='a-span9']/span\")\n",
    "        brand.append(brand_name.text)\n",
    "    except NoSuchElementException:\n",
    "        brand.append('-')    \n",
    "                \n",
    "      # getting the product name\n",
    "    try:\n",
    "        product=driver.find_element_by_id(\"productTitle\")\n",
    "        prod_name.append(product.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_name.append('-')         \n",
    "        \n",
    "      # getting the rating\n",
    "    try:\n",
    "        ratestar=driver.find_element_by_xpath(\"//span[@class='a-size-base a-nowrap']//span\")\n",
    "        rating.append(ratestar.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append(\"No Rating\")       \n",
    "                \n",
    "      # getting the rating number\n",
    "    try:\n",
    "        ratenum=driver.find_element_by_id(\"acrCustomerReviewText\")\n",
    "        rating_num.append(ratenum.text)\n",
    "    except NoSuchElementException:\n",
    "        rating_num.append('-')         \n",
    "                \n",
    "      # getting the price\n",
    "    try:\n",
    "        prod_price=driver.find_element_by_id(\"priceblock_ourprice\")\n",
    "        price.append(prod_price.text)\n",
    "    except NoSuchElementException:\n",
    "        price.append('-')      \n",
    "                \n",
    "     # getting the return/exchange\n",
    "    try:\n",
    "        ret_exch=driver.find_element_by_xpath(\"//a[@class='a-size-small a-link-normal a-text-normal']\")\n",
    "        ret_ex.append(ret_exch.get_attribute(\"href\"))\n",
    "    except NoSuchElementException:\n",
    "        ret_ex.append('-') \n",
    "        \n",
    "     # getting the expected delivery\n",
    "    try:\n",
    "        ex_de=driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-mini']/b\")\n",
    "        exp_del.append(ex_de.text)\n",
    "    except NoSuchElementException:\n",
    "        exp_del.append('-')           \n",
    "               \n",
    "     # getting the availability\n",
    "    try:\n",
    "        availa=driver.find_element_by_id(\"availability\")\n",
    "        availability.append(availa.text)\n",
    "    except NoSuchElementException:\n",
    "        availability.append('-')       \n",
    "                           \n",
    "     # getting the product url\n",
    "    try:\n",
    "        pro_url=driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-none icon-content']/a\")\n",
    "        prod_url.append(pro_url.get_attribute(\"href\"))\n",
    "    except NoSuchElementException:\n",
    "        prod_url.append('-')\n",
    "        \n",
    "     # getting the other details\n",
    "    try:\n",
    "        othr_det=driver.find_element_by_xpath(\"//ul[@class='a-unordered-list a-vertical a-spacing-mini']/li/span\")\n",
    "        ot_det.append(othr_det.text)\n",
    "    except NoSuchElementException:\n",
    "        ot_det.append('-')\n",
    "        \n",
    "# creating the entire dataframe now\n",
    "item_df = pd.DataFrame({})\n",
    "item_df['Brand Name'] = brand\n",
    "item_df['Product Name'] = prod_name\n",
    "item_df['Rating'] = rating\n",
    "item_df['Rating Number'] = rating_num\n",
    "item_df['Price'] = price\n",
    "item_df['Return/Exchange'] = ret_ex\n",
    "item_df['Expected Delivery'] = exp_del\n",
    "item_df['Availability'] = availability\n",
    "item_df['Product URLs'] = prod_url\n",
    "item_df['Other Details'] = ot_det     \n",
    "item_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3c2cd",
   "metadata": {},
   "source": [
    "#Q3-Write a python program to access the search bar and search button on images.google.com and scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "\n",
    "# Opening the google images\n",
    "url = \"https://images.google.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "search_bar = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/div/div[2]/input')    # Finding the search bar using it's xpath\n",
    "search_bar.send_keys(\"fruits\")       # Inputing \"fruits\" keyword to search images\n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/button')    # Finding the xpath of search button\n",
    "search_button.click()        # Clicking the search button\n",
    "\n",
    "\n",
    "# 500 time we scroll down by 10000 in order to generate more images on the website\n",
    "for _ in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")\n",
    "    \n",
    "images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"G:\\Fliprobo data\\fliprobo images\\fruits\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e4fdd",
   "metadata": {},
   "source": [
    "#Q4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Display Resolution”, “Processor”, “Processor Cores”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c60129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "\n",
    "# opning flipkart.com\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(3)\n",
    "try:\n",
    "    login_X_button = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]')                      # Button to close login popup\n",
    "    login_X_button.click()\n",
    "except NoSuchElementException : \n",
    "    print(\"No Login page\")\n",
    "    \n",
    "search_bar = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')    # Finding the search bar using it's xpath\n",
    "search_bar.clear()               # Clearing the search bar\n",
    "search_bar.send_keys(\"pixel\")       # Inputing keyword to search\n",
    "search_button = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')    # Finding the xpath of search button\n",
    "search_button.click()        # Clicking the search button\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6204621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching urls of phones coming on 1st page\n",
    "flip_urls = []\n",
    "urls = driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "for url in urls:\n",
    "    flip_urls.append(url.get_attribute(\"href\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "#Make empty lists   \n",
    "flip_dict = {}\n",
    "flip_dict[\"Brand\"] = []\n",
    "flip_dict[\"Smartphone\"] = []\n",
    "flip_dict[\"Colour\"] = []\n",
    "flip_dict[\"RAM\"] = []\n",
    "flip_dict[\"Storage(ROM)\"] = []\n",
    "flip_dict[\"Primary Camera\"] = []\n",
    "flip_dict[\"Secondary Camera\"] = []\n",
    "flip_dict[\"Display Size\"] = []\n",
    "flip_dict[\"Display Resolution\"] = []\n",
    "flip_dict[\"Processor\"] = []\n",
    "flip_dict[\"Processor Cores\"] = []\n",
    "flip_dict[\"Battery Capacity\"] = []\n",
    "flip_dict[\"Battery Type\"] = []\n",
    "flip_dict[\"Price\"] = []\n",
    "flip_dict[\"URL\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b30b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from each url\n",
    "for url in flip_urls:\n",
    "    driver.get(url)    # Saving url                                                     \n",
    "    print(\"Scraping URL = \", url)\n",
    "    flip_dict['URL'].append(url)                                                          # Loading the webpage by url\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        read_more = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')     # Button for expanding the specs\n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception Occured. Moving to next page\")\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')      # Extracting Brand from xpath\n",
    "        flip_dict[\"Brand\"].append(brand.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Brand'].append('-')\n",
    "        \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')      # Extracting Price from xpath\n",
    "        flip_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Price'].append('-')\n",
    "        \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')      # Extracting Name from xpath\n",
    "        flip_dict['Smartphone'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Smartphone'].append('-')\n",
    "    \n",
    "    try:\n",
    "        color = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      # Extracting colour from xpath\n",
    "        flip_dict['Colour'].append(color.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Colour'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_size = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[1]/td[2]/ul/li')  # Extracting Display Size from xpath\n",
    "        flip_dict['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Size'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_res = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[2]/td[2]/ul/li')     # Extracting Display Resolution from xpath\n",
    "        flip_dict['Display Resolution'].append(disp_res.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Resolution'].append('-')\n",
    "    \n",
    "    try:\n",
    "        pro_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "        if pro_chk.text != \"Processor Type\" : raise NoSuchElementException\n",
    "        processor = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')   # Extracting processor from xpath\n",
    "        Processor.append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        Processor.append('-')\n",
    "    \n",
    "    try:                                                                                     # Extracting Processor Cores from xpath\n",
    "        core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[1]')\n",
    "        if core_chk.text != \"Processor Core\" :\n",
    "            core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "            if core_chk.text != \"Processor Core\" : \n",
    "                raise NoSuchElementException\n",
    "            else :\n",
    "                cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        else :\n",
    "            cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[2]/ul/li')\n",
    "        flip_dict['Processor Cores'].append(cores.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor Cores'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li')         # Extracting Storage(ROM) from xpath\n",
    "        flip_dict['Storage(ROM)'].append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Storage(ROM)'].append('-')\n",
    "    \n",
    "    try:\n",
    "        ram = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                # Extracting RAM from xpath\n",
    "        flip_dict['RAM'].append(ram.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['RAM'].append('-')\n",
    "    \n",
    "    try:                                                                                  \n",
    "        pri_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li')     # Extracting Camera from xpath\n",
    "        flip_dict['Primary Camera'].append(pri_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Primary Camera'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Secondary Camera from xpath\n",
    "        cam_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[1]')\n",
    "        if cam_chk != \"Secondary Camera\" : \n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[1]').text == \"Secondary Camera\":\n",
    "                sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li')\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[2]/ul/li')\n",
    "        flip_dict['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Secondary Camera'].append('-')\n",
    "        \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :   # Extracting Battery Capacity from xpath\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[2]/ul/li')                \n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :     \n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[1]')\n",
    "            if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['Battery Capacity'].append(bat_cap.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Capacity'].append('-')\n",
    "    \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[1]')\n",
    "            if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "            bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[2]/ul/li')             # Extracting Battery Type from xpath\n",
    "        flip_dict['Battery Type'].append(bat_typ.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Type'].append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(flip_dict[\"Brand\"]), len(flip_dict[\"Smartphone\"]), len(flip_dict[\"Processor\"]), len(flip_dict[\"Price\"]), len(flip_dict['URL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_df = pd.DataFrame.from_dict(flip_dict)\n",
    "flip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a037937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening google maps\n",
    "url = \"https://www.google.co.in/maps\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "search = driver.find_element_by_id(\"searchboxinput\")                       # locating search bar\n",
    "search.clear()                                                             # clearing search bar\n",
    "time.sleep(2)\n",
    "search.send_keys(\"New Delhi\")                                                     # entering values in search bar\n",
    "button = driver.find_element_by_id(\"searchbox-searchbutton\")               # locating search button\n",
    "button.click()                                                             # clicking search button\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b97576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Write a program to scrap details of all the funding deals for second quarter (i.e. July 20 – September 20) from trak.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb156e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening trak.in\n",
    "url = \"https://trak.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# do click on funding deals\n",
    "button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(button)\n",
    "\n",
    "# Empty lists\n",
    "fund_dict = {}\n",
    "fund_dict['Date'] = []\n",
    "fund_dict['Startup Name'] = []\n",
    "fund_dict['Industry/Vertical'] = []\n",
    "fund_dict['Sub-Vertical'] = []\n",
    "fund_dict['Location'] = []\n",
    "fund_dict['Investor'] = []\n",
    "fund_dict['Investment Type'] = []\n",
    "fund_dict['Amount(in USD)'] = []\n",
    "\n",
    "\n",
    "for i in range(48,51):\n",
    "    # Date\n",
    "    dt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for d in dt:\n",
    "        fund_dict['Date'].append(d.text)\n",
    "\n",
    "    # Startup Name\n",
    "    sn = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for n in sn:\n",
    "        fund_dict['Startup Name'].append(n.text)\n",
    "    \n",
    "    # Industry/Vertical\n",
    "    ind = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind:\n",
    "        fund_dict['Industry/Vertical'].append(n.text)\n",
    "    \n",
    "    # Sub-Vertical\n",
    "    sv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for s in sv:\n",
    "        fund_dict['Sub-Vertical'].append(s.text)\n",
    "\n",
    "    # Location\n",
    "    loc = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for l in loc:\n",
    "        fund_dict['Location'].append(l.text)\n",
    "    \n",
    "    # Investor\n",
    "    inv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for n in inv:\n",
    "        fund_dict['Investor'].append(n.text)\n",
    "    \n",
    "    # Investment Type\n",
    "    invt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for n in invt:\n",
    "        fund_dict['Investment Type'].append(n.text)\n",
    "    \n",
    "    # Amount\n",
    "    amt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for a in amt:\n",
    "        fund_dict['Amount(in USD)'].append(a.text)\n",
    "    \n",
    "fund_df = pd.DataFrame(fund_dict)\n",
    "fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3681a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Write a program to scrap all the available details of top 10 gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening www.digit.in\n",
    "url = \"https://www.digit.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "#clicking on top 10 option \n",
    "top_10=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[4]/ul/li[4]/a\")\n",
    "top_10.click()\n",
    "\n",
    "time.sleep(2)\n",
    "#clicking on laptops option\n",
    "laptops=driver.find_element_by_xpath(\"/html/body/div[3]/div/div/div[2]/div[5]/div[1]/div/button[2]\")\n",
    "laptops.click()\n",
    "\n",
    "time.sleep(2)\n",
    "#best gaming laptops link\n",
    "best_gaming=driver.find_element_by_xpath(\"//div[@id='laptops']//div[3]//a\")\n",
    "driver.get(best_gaming.get_attribute('href'))\n",
    "\n",
    "#intialising lists\n",
    "name = []\n",
    "Price = []\n",
    "OS = []\n",
    "display = []\n",
    "processor = []\n",
    "HDD = []\n",
    "RAM = []\n",
    "weight = []\n",
    "dimension = []\n",
    "GPU = []\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping names\n",
    "names=driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for i in names:\n",
    "    name.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping operating system\n",
    "os=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "for i in os:\n",
    "    OS.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping display\n",
    "displays=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "for i in displays:\n",
    "    display.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping processor\n",
    "processors=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[3]/div/div\")\n",
    "for i in processors:\n",
    "    processor.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping memory\n",
    "memories=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")# extrat HDD and RAM form xpath\n",
    "for i in memories:\n",
    "    HDD.append(i.text.split(\"/\")[0])\n",
    "    RAM.append(i.text.split(\"/\")[1])\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping weight\n",
    "weights=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")# extrat weight form xpath\n",
    "for i in weights:\n",
    "    weight.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping dimension\n",
    "dimension=[]\n",
    "dimensions=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\") \n",
    "for i in dimensions:\n",
    "    dimension.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping graphical processor\n",
    "GPUs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[9]/td[3]\") \n",
    "for i in GPUs:\n",
    "    GPU.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#scraping price\n",
    "price=driver.find_elements_by_xpath(\"//table[@id='summtable']//tr//td[3]\")\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "\n",
    "#Make data frame\n",
    "df=pd.DataFrame({\"Name\":name,\n",
    "                \"price\":Price,\n",
    "                \"OS\":OS,\n",
    "                \"Display\":display,\n",
    "                \"HDD\":HDD,\n",
    "                 \"RAM\":RAM,\n",
    "                \"processor\":processor,\n",
    "                \"weight\":weight,\n",
    "                \"Dimension\":dimension,\n",
    "                \"Graphical processor\":GPU})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Gaming laptops_digit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e179d3e",
   "metadata": {},
   "source": [
    "#Q8- Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381554b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# Opening the forbes.com\n",
    "url = \"https://www.forbes.com/?sh=69e6b8c92254\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "#clicking the explore button\n",
    "button = driver.find_element_by_xpath(\"//button[@class='icon--hamburger']\")\n",
    "button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#select billionaire  \n",
    "billioners = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "billioners.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#select world billionaire  \n",
    "world_billioners= driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "world_billioners.click()\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "#make empty listes\n",
    "Rank = [] \n",
    "Person_Name = [] \n",
    "total_net_worth = [] \n",
    "Age = []\n",
    "country_of_citizenship = [] \n",
    "Source = []\n",
    "industry = []\n",
    "\n",
    "\n",
    "while(True):\n",
    "    #scraping rank of billionaire \n",
    "    rank= driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "    for i in rank:\n",
    "        Rank.append(i.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #scraping name of billionaire \n",
    "    name= driver.find_elements_by_xpath(\"//div[@class='personName']//div\")\n",
    "    for i in name:\n",
    "        Person_Name.append(i.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "     #scraping Age of billionaire \n",
    "    age= driver.find_elements_by_xpath(\"//div[@class='age']//div\")\n",
    "    for i in age:\n",
    "        Age.append(i.text)   \n",
    "    time.sleep(1)\n",
    "    \n",
    "    #scraping citizenship of billionaire     \n",
    "    citizenship= driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "    for i in citizenship:\n",
    "        country_of_citizenship.append(i.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #scraping source of income\n",
    "    source= driver.find_elements_by_xpath(\"//div[@class='source']\")\n",
    "    for i in source:\n",
    "        Source.append(i.text)    \n",
    "    time.sleep(1)\n",
    "        \n",
    "    #scraping Age of billionaire \n",
    "    industries= driver.find_elements_by_xpath(\"//div[@class='category']//div\")\n",
    "    for i in industries:\n",
    "        industry.append(i.text)\n",
    "        \n",
    "    #scraping net_worth of billionaire \n",
    "    net_worth= driver.find_elements_by_xpath(\"//div[@class='netWorth']//div[1]\")\n",
    "    for i in net_worth:\n",
    "        total_net_worth.append(i.text)\n",
    "    time.sleep(1)    \n",
    "        \n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath(\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break  \n",
    "\n",
    "        \n",
    "Net_Worth = []        \n",
    "for i in range(0,len(total_net_worth),2):\n",
    "        Net_Worth.append(total_net_worth[i])\n",
    "    \n",
    "\n",
    "time.sleep(2)        \n",
    "#creating dataframe\n",
    "df=pd.DataFrame({'Rank':Rank,\n",
    "                'Name':Person_Name,\n",
    "                'Net Worth':Net_Worth,\n",
    "                'Age':Age,\n",
    "                'Country of Citizenship':country_of_citizenship,\n",
    "                'Source':Source,\n",
    "                'Industry':industry})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd90822",
   "metadata": {},
   "source": [
    "#Q9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60024b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Opening the youtube.com\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "#finding element for search bar\n",
    "search_bar = driver.find_element_by_id('search')\n",
    "search_bar.send_keys(\"GOT\")  #entering Video name\n",
    "time.sleep(1)\n",
    "\n",
    "#clicking on search button\n",
    "search_btn = driver.find_element_by_id(\"search-icon-legacy\")  \n",
    "search_btn.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#clicking on first video\n",
    "link_click = driver.find_element_by_xpath(\"//yt-formatted-string[@class ='style-scope ytd-video-renderer']\")\n",
    "link_click.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4381c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 time we scroll down by 10000 in order to generate more Comments\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aad394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make empty lists\n",
    "comments = []\n",
    "comment_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []\n",
    "\n",
    "#scrape comments\n",
    "cm = driver.find_elements_by_id(\"content-text\")\n",
    "for i in cm:\n",
    "    if i.text is None:\n",
    "        comments.append(\"--\")\n",
    "    else:\n",
    "        comments.append(i.text)\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# scrape time when comment was posted\n",
    "tm = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "for i in tm:\n",
    "    Time.append(i.text)\n",
    "\n",
    "for i in range(0,len(Time),2):\n",
    "    comment_time.append(Time[i])\n",
    "time.sleep(5)\n",
    "    \n",
    "# scrape the comment likes\n",
    "like = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])\n",
    "    \n",
    "    \n",
    "time.sleep(2)\n",
    "#creating dataframe\n",
    "df=pd.DataFrame({'Comments':comments,\n",
    "                'Time':comment_time,\n",
    "                'Likes':No_of_Likes})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf7afe",
   "metadata": {},
   "source": [
    "#Q10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "\n",
    "# Opening the homepage of hostelworld.com\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the location search bar\n",
    "search_loc = driver.find_element_by_id('search-input-field')\n",
    "# write Lonodn in search bar\n",
    "search_loc.send_keys(\"London\")\n",
    "time.sleep(2)\n",
    "\n",
    "#select london\n",
    "london = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/div[1]/div/div[2]/div[4]/div/div[2]/div/div[1]/div/div/ul/li[2]/div')\n",
    "london.click()\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "# do click on search button\n",
    "search_btn = driver.find_element_by_id('search-button')\n",
    "search_btn.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make empty lists\n",
    "Hostel_Name = []\n",
    "Distance = []\n",
    "overall_review = []\n",
    "total_reviews = []\n",
    "facilities = []\n",
    "price = []\n",
    "Rating = []\n",
    "property_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a3e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    \n",
    "    # Hostel name\n",
    "    names = driver.find_elements_by_xpath('//h2[@class=\"title title-6\"]')\n",
    "    for name in names:\n",
    "        Hostel_Name.append(name.text)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    # Distance from city centre\n",
    "    dis = driver.find_elements_by_xpath('//span[@class=\"description\"]')\n",
    "    for d in dis:\n",
    "        Distance.append(d.text)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    # Overall Review    \n",
    "    review = driver.find_elements_by_xpath('//div[@class=\"keyword\"]//span')\n",
    "    for r in review:\n",
    "        overall_review.append(r.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Total No of reviews     \n",
    "    t_review = driver.find_elements_by_xpath('//div[@class=\"reviews\"]')\n",
    "    for tr in t_review:\n",
    "        total_reviews.append(tr.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # facilities\n",
    "    service = driver.find_elements_by_xpath('//div[@class=\"facilities-label facilities\"]')\n",
    "    for s in service:\n",
    "        facilities.append(s.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Prices    \n",
    "    prices = driver.find_elements_by_xpath('//div[@class=\"price-col\"]')\n",
    "    for p in prices:\n",
    "        price.append(p.text)\n",
    "    time.sleep(2)    \n",
    "        \n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath('//div[@class=\"pagination-item pagination-next\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "time.sleep(4)        \n",
    "        \n",
    "# Separate  Privates_From price  and  Dorms_From price\n",
    "Privates_From = []\n",
    "for i in range(0,len(price),2):\n",
    "    Privates_From.append(price[i])\n",
    "time.sleep(2)\n",
    "\n",
    "Dorms_From = []\n",
    "for i in range(1,len(price),2):\n",
    "    Dorms_From.append(price[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da44802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape Hostels URL\n",
    "hostel_url = []\n",
    "\n",
    "while(True):\n",
    "    urls = driver.find_elements_by_xpath('//h2[@class=\"title title-6\"]//a')\n",
    "    for url in urls:\n",
    "        hostel_url.append(url.get_attribute(\"href\"))\n",
    "    time.sleep(2)    \n",
    "        \n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath('//div[@class=\"pagination-item pagination-next\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "\n",
    "\n",
    "        \n",
    "Rate = []\n",
    "for page in hostel_url:\n",
    "    driver.get(page)\n",
    "    \n",
    "    # Rating\n",
    "    try:\n",
    "        ratings = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/section/div[6]/div/div[1]/div[1]/div[1]')\n",
    "        Rate.append(ratings.text)\n",
    "    except NoSuchElementException:\n",
    "        Rate.append(\"No Rating\")  \n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    # Property Description\n",
    "    try:\n",
    "        pd = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/section/div[6]/div/div[2]/div[2]/div/div[2]')\n",
    "        property_description.append(pd.text)\n",
    "    except NoSuchElementException:\n",
    "        property_description.append(\"No Description\")  \n",
    "\n",
    "    \n",
    "time.sleep(2)        \n",
    "# remove extra data from Rating     \n",
    "all_text = []\n",
    "for i in Rate:\n",
    "    all_text.append(i.split())\n",
    "time.sleep(2)\n",
    "\n",
    "for i in all_text:\n",
    "    Rating.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "df = pd.DataFrame({'Hostel Name':Hostel_Name,\n",
    "                'Distance from city centre':Distance,\n",
    "                'Overall Review':overall_review[3:],\n",
    "                'Total Reviews':total_reviews,\n",
    "                'Facilities':facilities,\n",
    "                'Privates From Price':Privates_From,\n",
    "                'Dorms From Price':Dorms_From,\n",
    "                'Rating':Rating,\n",
    "                'Property Description':property_description})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd4af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
